{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Метрики бинарной классификации\n",
    "\n",
    "**P (positive)** - истинное количество позитивных случаев / больных / объектов, относящихся к классу 1\n",
    "\n",
    "**N (negative)** - истинное количество негативных случаев / здоровых / объектов, относящихся к классу 0\n",
    "\n",
    "**TP (true positive)** - количество правильно угаданных позитивных случаев 1 ✅\n",
    "\n",
    "**TN (true negative)** - количество правильно угаданных негативных случаев 0 ✅\n",
    "\n",
    "**FP (false positive)** - количество неправильно угаданных негативных случаев 0 ❌ (ошибка первого рода)\n",
    "\n",
    "**FN (false negative)** - количество неправильно угаданных позитивных случаев 1 ❌ (ошибка второго рода)\n",
    "\n",
    "**Чувствитльность (sensitivity), полнота (recall), true positive rate (TPR)**:\n",
    "\n",
    "$$ TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN} = \\frac{1 ✅}{1 ✅ + 1 ❌}$$\n",
    "\n",
    "**Специфичность (specificity), true negative rate**:\n",
    "\n",
    "$$ TNR = \\frac{TN}{N} = \\frac{TN}{TN + FP} = \\frac{0 ✅}{0 ✅ + 0 ❌}$$\n",
    "\n",
    "**Точность-меткость (precision), positive predictive value (PPV)**:\n",
    "\n",
    "$$ PPV = \\frac{TP}{TP + FP} = \\frac{1 ✅}{1 ✅ + 0 ❌} $$\n",
    "\n",
    "**Точность (accuracy) (ACC)**:\n",
    "\n",
    "$$ ACC = \\frac{TP + TN}{P + N} = \\frac{1 ✅ + 0 ✅}{1 ✅ + 1 ❌ + 0 ✅ + 0 ❌} $$\n",
    "\n",
    "![accprecc](https://www.researchgate.net/profile/Anni-Helena-Ruotsala/publication/304674901/figure/fig6/AS:668649476067338@1536429866393/Precision-versus-accuracy-The-bullseye-represents-the-true-value-eg-the-true.ppm)\n",
    "\n",
    "**F1 метрика**\n",
    "\n",
    "F1 метрика - способ объединить precision (точность-меткость) и recall (чувствительность) через среднее гармоническое.\n",
    "\n",
    "Цель объединения - использование двух метрик сразу при обучении модели и поиска лучших гиперпараметров.\n",
    "\n",
    "$$ F1 = \\frac{2}{\\frac{1}{precision} + \\frac{1}{recall}} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import (\n",
    "    preprocessing,\n",
    "    linear_model,\n",
    "    svm,\n",
    "    neighbors,\n",
    "    naive_bayes,\n",
    "    ensemble,\n",
    "    neural_network,\n",
    "    model_selection,\n",
    "    pipeline,\n",
    "    feature_selection,\n",
    "    metrics,\n",
    ")\n",
    "from featuretools import selection\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cells = pd.read_table('T_cells.tsv')\n",
    "b_cells = pd.read_table('B_cells.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preporcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cells_modified = t_cells.drop(t_cells.columns[0], axis=1)\n",
    "t_cells_modified['CELL_TYPE'] = 1\n",
    "\n",
    "b_cells_modified = b_cells.drop(b_cells.columns[0], axis=1)\n",
    "b_cells_modified['CELL_TYPE'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells = t_cells_modified.copy(deep=True)\n",
    "all_cells = all_cells.append(b_cells_modified).reset_index(drop=True)\n",
    "all_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normilize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_X = all_cells.iloc[: , :-1]\n",
    "temp_X = preprocessing.normalize(all_cells_X, norm='l2')\n",
    "all_cells_X = pd.DataFrame(temp_X, columns=all_cells_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_cells = selection.remove_highly_correlated_features(all_cells_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_all_cells.columns.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "y_all_cells = all_cells.iloc[: , -1:].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X_all_cells.values, y_all_cells, test_size=TEST_SIZE, random_state=np.random.RandomState(0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIERS = {\n",
    "    # Logistic Regression.\n",
    "    linear_model.LogisticRegression(): {'C': [1, 5, 10]},\n",
    "    \n",
    "    # Perceptron.\n",
    "    linear_model.Perceptron(\n",
    "        penalty='elasticnet', early_stopping=True\n",
    "    ): {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]},\n",
    "    \n",
    "    # Support-vector machine.\n",
    "    svm.SVC(): {\n",
    "        'kernel': ['rbf', 'linear'], 'C': [1, 5, 10]\n",
    "    },\n",
    "    \n",
    "    # k-nearest Neighbors algorithm.\n",
    "    neighbors.KNeighborsClassifier(): {\n",
    "        'n_neighbors': [5, 10, 15, 20], 'weights': ('uniform', 'distance')\n",
    "    },\n",
    "    \n",
    "    # Gaussian Naive Bayes.\n",
    "    naive_bayes.GaussianNB(): None,\n",
    "    \n",
    "    # Gradient Boosting.\n",
    "    ensemble.GradientBoostingClassifier(): {\n",
    "        'learning_rate': [0.01, 0.1, 1.0], 'n_estimators': [10, 100, 1000]\n",
    "    },\n",
    "    \n",
    "    # Multilayer Perceptron.\n",
    "    neural_network.MLPClassifier(learning_rate='adaptive', early_stopping=True): None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best features finding using grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_FEATURES_RANGE = 3\n",
    "\n",
    "scores = ['accuracy', 'f1']  # Classifiers scores for grid search\n",
    "all_best_features_counts = {}\n",
    "\n",
    "for k in range(1, N_FEATURES_RANGE+1):\n",
    "    # ANOVA F-value scoring function.\n",
    "    SCORE_FUNCTION = feature_selection.f_classif\n",
    "\n",
    "    # Number of selected features.\n",
    "    N_FEAUTRES = k\n",
    "\n",
    "    # Select features according to the k highest scores.\n",
    "    skbest = feature_selection.SelectKBest(\n",
    "        score_func=SCORE_FUNCTION,\n",
    "        k=N_FEAUTRES\n",
    "    )\n",
    "    # Iterate over all classifiers.\n",
    "    current_best_features_counts = {}\n",
    "    for classifier, tuned_parameters in CLASSIFIERS.items():\n",
    "        for score in scores:\n",
    "            # Init Pipeline.\n",
    "            pipe = pipeline.Pipeline(steps=[\n",
    "                ('filter', skbest),\n",
    "                ('classifier', classifier),\n",
    "            ])\n",
    "\n",
    "            # Init parameters for grid search.\n",
    "            param_grid = {}\n",
    "            if tuned_parameters is not None:\n",
    "                for param_name, values in tuned_parameters.items():\n",
    "                    param_grid[f'classifier__{param_name}'] = values\n",
    "\n",
    "            # Preapare cross-validation method.\n",
    "            cv = model_selection.ShuffleSplit(\n",
    "                n_splits=5,\n",
    "                test_size=0.2,\n",
    "                random_state=np.random.RandomState(0)\n",
    "            )\n",
    "\n",
    "            # Run grid search.\n",
    "            search = model_selection.GridSearchCV(\n",
    "                pipe,\n",
    "                param_grid,\n",
    "                scoring=score,\n",
    "                cv=cv,\n",
    "                n_jobs=-1,\n",
    "            )\n",
    "            search.fit(X_train, y_train)\n",
    "\n",
    "            # Add best features.\n",
    "            best_features_names = list(feature_names[\n",
    "                search.best_estimator_['filter'].get_support()\n",
    "            ])\n",
    "            for feature_name in best_features_names:\n",
    "                if feature_name not in current_best_features_counts:\n",
    "                    current_best_features_counts[feature_name] = 1\n",
    "                else:\n",
    "                    current_best_features_counts[feature_name] += 1\n",
    "    all_best_features_counts[N_FEAUTRES] = current_best_features_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Found features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_best_features_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "LST = []\n",
    "LABELS = set()\n",
    "\n",
    "for i in range(N_FEATURES_RANGE):\n",
    "    spec_features = all_best_features_counts[i+1]\n",
    "    selected_feature_names = list(spec_features.keys())\n",
    "    features_scores = list(spec_features.values())\n",
    "    LABELS = LABELS.union(set(spec_features.keys()))\n",
    "\n",
    "width = 0.2\n",
    "x = np.arange(len(list(LABELS)))\n",
    "for i in range(N_FEATURES_RANGE):\n",
    "    temp_lst = [0 for i in range(N_FEATURES_RANGE)]\n",
    "    features_scores = list(all_best_features_counts[i+1].values())\n",
    "    features_scores = features_scores + temp_lst[i+1:]\n",
    "    rects = ax.bar(x - width*i, features_scores, width, color=np.random.random(3), label=i+1)\n",
    "    LST.append(rects)\n",
    "\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Feature counts from different numbers of festures to select')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(list(LABELS))\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test best features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_features(fueatures_names, df_to_use):\n",
    "    # Cut data by features.\n",
    "    X_all_cells_cut = df_to_use[fueatures_names]\n",
    "\n",
    "    # Preapare data for testing.\n",
    "    X_train_eval, X_test_eval, y_train_eval, y_test_eval = model_selection.train_test_split(\n",
    "        X_all_cells_cut.values, y_all_cells, test_size=0.2, random_state=np.random.RandomState(0),\n",
    "    )\n",
    "\n",
    "    test_results = []\n",
    "    for classifier, tuned_parameters in CLASSIFIERS.items():\n",
    "        # Preapare cross-validation method.\n",
    "        cv = model_selection.ShuffleSplit(\n",
    "            n_splits=5,\n",
    "            test_size=0.2,\n",
    "            random_state=np.random.RandomState(0)\n",
    "        )\n",
    "\n",
    "        # Prepare grid search.\n",
    "        param_grid = {}\n",
    "        if tuned_parameters is not None:\n",
    "            for param_name, values in tuned_parameters.items():\n",
    "                param_grid[f'{param_name}'] = values\n",
    "        test_search = model_selection.GridSearchCV(\n",
    "            classifier,\n",
    "            param_grid,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        test_search.fit(X_train_eval, y_train_eval)\n",
    "        \n",
    "        y_pred_eval_train = test_search.predict(X_train_eval)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(\n",
    "            y_train_eval, y_pred_eval_train\n",
    "        ).ravel()\n",
    "        TRAIN_SENS = round(tp / (tp + fn), 2)\n",
    "        TRAIN_SPEC = round(tn / (tn + fp), 2)\n",
    "        TRAIN_ACC = round(metrics.accuracy_score(y_train_eval, y_pred_eval_train), 2)\n",
    "        \n",
    "        y_pred_eval_test = test_search.predict(X_test_eval)\n",
    "        tn, fp, fn, tp = metrics.confusion_matrix(\n",
    "            y_test_eval, y_pred_eval_test\n",
    "        ).ravel()\n",
    "        TEST_SENS = round(tp / (tp + fn), 2)\n",
    "        TEST_SPEC = round(tn / (tn + fp), 2)\n",
    "        TEST_ACC = round(metrics.accuracy_score(y_test_eval, y_pred_eval_test), 2)\n",
    "\n",
    "        test_results.append({\n",
    "            'classifier': classifier.__class__.__name__,\n",
    "            'TRAIN sens': TRAIN_SENS,\n",
    "            'TRAIN spec': TRAIN_SPEC,\n",
    "            'TRAIN ACC': TRAIN_ACC,\n",
    "            'TEST sens': TEST_SENS,\n",
    "            'TEST spec': TEST_SPEC,\n",
    "            'TEST ACC': TEST_ACC, \n",
    "        })\n",
    "    test_results_df = pd.DataFrame(\n",
    "        test_results).set_index('classifier').sort_values(\n",
    "            by=['TEST ACC', 'TRAIN ACC'], ascending=False\n",
    "    )\n",
    "    return (X_all_cells_cut, test_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Best 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_names = list(all_best_features_counts[1].keys())\n",
    "slice_df_1, test_results_1 = eval_features(best_features_names, X_all_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selected features: ', list(slice_df_1.columns))\n",
    "test_results_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Best 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_names = list(all_best_features_counts[2].keys())\n",
    "slice_df_2, test_results_2 = eval_features(best_features_names, X_all_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selected features: ', list(slice_df_2.columns))\n",
    "test_results_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Best 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_names = list(all_best_features_counts[3].keys())\n",
    "slice_df_3, test_results_3 = eval_features(best_features_names, X_all_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Selected features: ', list(slice_df_3.columns))\n",
    "test_results_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Combination given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_features = {\n",
    "    482: 'CMY_Y_Wavelet_energy',\n",
    "    66: 'RGB_B_Wavelet_energy',\n",
    "    79: 'HSVIL_H_Матрица простр смежности_ASM',\n",
    "    54: 'RGB_B_Матрица простр смежности_CON',\n",
    "    466: 'CMY_M_pix_mean',\n",
    "    83: 'HSVIL_H_Матрица простр смежности_MPR',\n",
    "    77: 'RGB_B_pix_stddev',\n",
    "    492: 'CMY_Y_pix_mean',\n",
    "    2: 'RGB_R_Матрица простр смежности_CON',\n",
    "    51: 'RGB_G_pix_stddev',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_1 = []\n",
    "for feature in given_features.values():\n",
    "    slice_df_given_1, test_results_given_1 = eval_features([feature], all_cells_X)\n",
    "    item = {\n",
    "        'FEATURE_NAME': feature,\n",
    "        'best classifier': test_results_given_1.index[0],\n",
    "    }\n",
    "    item_keys = list(test_results_given_1.iloc[0].index)\n",
    "    item_values = list(test_results_given_1.iloc[0].values)\n",
    "    item = {**item, **dict(zip(item_keys, item_values))}\n",
    "    given_1.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_given_1 = pd.DataFrame(given_1).set_index('FEATURE_NAME')\n",
    "results_given_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_given_1['TEST ACC'].plot.barh(title='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_2 = [\n",
    "    [66, 79],\n",
    "    [482, 79],\n",
    "    [482, 492],\n",
    "    [66, 83],\n",
    "    [482, 77],\n",
    "    [66, 54],\n",
    "]\n",
    "features_2 = [[given_features[pair[0]], given_features[pair[1]]] for pair in features_2]\n",
    "features_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_2 = []\n",
    "for features_pair in features_2:\n",
    "    slice_df_given_2, test_results_given_2 = eval_features(features_pair, all_cells_X)\n",
    "    item = {\n",
    "        'FEATURE_PAIR': features_pair,\n",
    "        'best classifier': test_results_given_2.index[0],\n",
    "    }\n",
    "    item_keys = list(test_results_given_2.iloc[0].index)\n",
    "    item_values = list(test_results_given_2.iloc[0].values)\n",
    "    item = {**item, **dict(zip(item_keys, item_values))}\n",
    "    given_2.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_given_2 = pd.DataFrame(given_2).set_index('FEATURE_PAIR')\n",
    "results_given_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_given_2['TEST ACC'].plot.barh(title='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_3 = [\n",
    "    [66, 79, 466],\n",
    "    [482, 79, 54],\n",
    "    [66, 79, 51],\n",
    "    [66, 83, 2],\n",
    "]\n",
    "features_3 = [[\n",
    "    given_features[tiple[0]],\n",
    "    given_features[tiple[1]],\n",
    "    given_features[tiple[2]]\n",
    "] for tiple in features_3]\n",
    "features_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_3 = []\n",
    "for features_triple in features_3:\n",
    "    slice_df_given_3, test_results_given_3 = eval_features(features_triple, all_cells_X)\n",
    "    item = {\n",
    "        'FEATURE_TRIPLE': features_pair,\n",
    "        'best classifier': test_results_given_3.index[0],\n",
    "    }\n",
    "    item_keys = list(test_results_given_3.iloc[0].index)\n",
    "    item_values = list(test_results_given_3.iloc[0].values)\n",
    "    item = {**item, **dict(zip(item_keys, item_values))}\n",
    "    given_3.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_given_3 = pd.DataFrame(given_3).set_index('FEATURE_TRIPLE')\n",
    "results_given_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = results_given_3['TEST ACC'].plot.barh(title='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "user_python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
